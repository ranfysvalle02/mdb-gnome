#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Intelligent Standalone FastAPI Application for experiment '{{ slug_id }}'.

This is a clean, production-ready FastAPI application that:
- Uses real MongoDB (Motor) for persistent storage
- REQUIRES Ray - Ray is a core component and must be available
- Includes proper database initialization and index management
- Supports experiment-specific routes and static files

Environment Variables:
- MONGO_URI: MongoDB connection string (default: mongodb://mongo:27017/)
- DB_NAME: Database name (default: labs_db)
- PORT: Server port (default: 8000)
- LOG_LEVEL: Logging level (default: INFO)
- RAY_CONNECTION_ADDRESS: (Optional) Ray cluster address. If not set, starts local Ray.
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request, HTTPException, status, Depends
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

# MongoDB imports
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase

# Import experiment-specific dependencies
try:
    from async_mongo_wrapper import ScopedMongoWrapper, AsyncAtlasIndexManager
    MONGO_WRAPPER_AVAILABLE = True
except ImportError:
    MONGO_WRAPPER_AVAILABLE = False
    logging.warning("async_mongo_wrapper not available. Using direct MongoDB access.")

# --------------------------------------------------------------------------
# Logging Configuration
# --------------------------------------------------------------------------
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO").upper(),
    format="%(asctime)s | standalone | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("standalone")

# --------------------------------------------------------------------------
# Constants and Paths
# --------------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent
SLUG = "{{ slug_id }}"
EXPERIMENT_DIR = BASE_DIR / "experiments" / SLUG

# --------------------------------------------------------------------------
# Environment Configuration
# --------------------------------------------------------------------------
MONGO_URI = os.getenv("MONGO_URI", "mongodb://mongo:27017/")
DB_NAME = os.getenv("DB_NAME", "labs_db")
PORT = int(os.getenv("PORT", "8000"))

# --------------------------------------------------------------------------
# Load Configuration from JSON files
# --------------------------------------------------------------------------
def _load_json(path: Path, default: Any) -> Any:
    try:
        with path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        logger.warning(f"JSON file not found: {path.name}. Using default.")
    except Exception as e:
        logger.error(f"Failed to read '{path.name}': {e}")
    return default

DB_CONFIG: Dict[str, Any] = _load_json(BASE_DIR / "db_config.json", {})
DB_COLLECTIONS: Dict[str, List[Dict[str, Any]]] = _load_json(BASE_DIR / "db_collections.json", {})

# --------------------------------------------------------------------------
# MongoDB Connection and Database Setup
# --------------------------------------------------------------------------
mongo_client: Optional[AsyncIOMotorClient] = None
mongo_db: Optional[AsyncIOMotorDatabase] = None

async def connect_mongodb():
    """Initialize MongoDB connection."""
    global mongo_client, mongo_db
    try:
        logger.info(f"Connecting to MongoDB at '{MONGO_URI}'...")
        mongo_client = AsyncIOMotorClient(
            MONGO_URI,
            serverSelectionTimeoutMS=5000,
            appname=f"Standalone-{SLUG}"
        )
        await mongo_client.admin.command("ping")
        mongo_db = mongo_client[DB_NAME]
        logger.info(f"MongoDB connection successful (Database: '{DB_NAME}').")
        
        # Seed database with exported data if collections are empty
        await seed_database()
        
        # Ensure indexes from manifest
        await ensure_indexes()
        
    except Exception as e:
        logger.critical(f"CRITICAL ERROR: Failed to connect to MongoDB: {e}", exc_info=True)
        raise RuntimeError(f"MongoDB connection failed: {e}") from e

async def close_mongodb():
    """Close MongoDB connection."""
    global mongo_client
    if mongo_client is not None:
        mongo_client.close()
        logger.info("MongoDB connection closed.")

async def seed_database():
    """Seed database collections with exported data."""
    if mongo_db is None:
        return
    
    for collection_name, docs in DB_COLLECTIONS.items():
        try:
            collection = mongo_db[collection_name]
            existing_count = await collection.count_documents({})
            if existing_count == 0 and docs:
                logger.info(f"Seeding collection '{collection_name}' with {len(docs)} documents...")
                # Convert string _id back to ObjectId if needed
                for doc in docs:
                    if "_id" in doc and isinstance(doc["_id"], str):
                        try:
                            from bson import ObjectId
                            doc["_id"] = ObjectId(doc["_id"])
                        except Exception:
                            pass  # Keep as string if ObjectId conversion fails
                await collection.insert_many(docs)
                logger.info(f"Successfully seeded '{collection_name}'.")
        except Exception as e:
            logger.error(f"Error seeding collection '{collection_name}': {e}", exc_info=True)

async def ensure_indexes():
    """Ensure MongoDB indexes from experiment manifest."""
    if mongo_db is None or not MONGO_WRAPPER_AVAILABLE:
        return
    
    if "managed_indexes" not in DB_CONFIG:
        return
    
    try:
        managed_indexes = DB_CONFIG.get("managed_indexes", {})
        for collection_name, index_defs in managed_indexes.items():
            # Construct the full collection name (with experiment prefix)
            full_collection_name = f"{SLUG}_{collection_name}"
            collection = mongo_db[full_collection_name]
            
            for index_def in index_defs:
                base_index_name = index_def.get("name", "unnamed_index")
                # Prefix index name with experiment slug to match actor's expectation
                index_name = f"{SLUG}_{base_index_name}"
                index_type = index_def.get("type", "search")
                definition = index_def.get("definition", {})
                
                if index_type == "vectorSearch":
                    # Use AsyncAtlasIndexManager for vector search indexes
                    manager = AsyncAtlasIndexManager(collection)
                    
                    # Check if index already exists
                    existing_index = await manager.get_search_index(index_name)
                    if existing_index:
                        current_def = existing_index.get("latestDefinition", existing_index.get("definition"))
                        
                        # Normalize both definitions for order-insensitive comparison
                        def _normalize_json_def(obj):
                            try:
                                return json.loads(json.dumps(obj, sort_keys=True))
                            except (TypeError, ValueError):
                                return obj
                        
                        normalized_current = _normalize_json_def(current_def)
                        normalized_expected = _normalize_json_def(definition)
                        
                        if normalized_current == normalized_expected:
                            logger.info(f"Vector search index '{index_name}' definition matches. Already exists.")
                            if not existing_index.get("queryable") and existing_index.get("status") != "FAILED":
                                logger.info(f"Index '{index_name}' not queryable yet; waiting...")
                                await manager._wait_for_search_index_ready(index_name, 30)
                        else:
                            # Index definition changed - update it
                            current_paths = [f.get('path', '?') for f in (normalized_current.get('fields', []) if isinstance(normalized_current, dict) else []) if isinstance(f, dict)]
                            expected_paths = [f.get('path', '?') for f in (normalized_expected.get('fields', []) if isinstance(normalized_expected, dict) else []) if isinstance(f, dict)]
                            logger.warning(f"Vector search index '{index_name}' definition changed. Updating...")
                            logger.info(f"Current filter fields: {current_paths}")
                            logger.info(f"Expected filter fields: {expected_paths}")
                            await manager.update_search_index(name=index_name, definition=definition, wait_for_ready=True)
                            logger.info(f"âœ… Updated vector search index '{index_name}'.")
                    else:
                        # Index doesn't exist - create it
                        await manager.create_search_index(
                            name=index_name,
                            definition=definition,
                            index_type="vectorSearch",
                            wait_for_ready=True
                        )
                        logger.info(f"Created vector search index '{index_name}' on '{full_collection_name}'.")
                elif index_type == "search":
                    # Lucene search index
                    manager = AsyncAtlasIndexManager(collection)
                    
                    # Check if index already exists
                    existing_index = await manager.get_search_index(index_name)
                    if existing_index:
                        current_def = existing_index.get("latestDefinition", existing_index.get("definition"))
                        
                        # Normalize both definitions for order-insensitive comparison
                        def _normalize_json_def(obj):
                            try:
                                return json.loads(json.dumps(obj, sort_keys=True))
                            except (TypeError, ValueError):
                                return obj
                        
                        normalized_current = _normalize_json_def(current_def)
                        normalized_expected = _normalize_json_def(definition)
                        
                        if normalized_current == normalized_expected:
                            logger.info(f"Search index '{index_name}' definition matches. Already exists.")
                            if not existing_index.get("queryable") and existing_index.get("status") != "FAILED":
                                logger.info(f"Index '{index_name}' not queryable yet; waiting...")
                                await manager._wait_for_search_index_ready(index_name, 30)
                        else:
                            # Index definition changed - update it
                            logger.warning(f"Search index '{index_name}' definition changed. Updating...")
                            await manager.update_search_index(name=index_name, definition=definition, wait_for_ready=True)
                            logger.info(f"âœ… Updated search index '{index_name}'.")
                    else:
                        # Index doesn't exist - create it
                        await manager.create_search_index(
                            name=index_name,
                            definition=definition,
                            index_type="search",
                            wait_for_ready=True
                        )
                        logger.info(f"Created search index '{index_name}' on '{full_collection_name}'.")
                else:
                    # Standard MongoDB index - use prefixed index name
                    try:
                        keys = index_def.get("keys", {})
                        
                        # Check if this is an _id index (MongoDB creates these automatically)
                        is_id_index = False
                        if isinstance(keys, dict):
                            is_id_index = len(keys) == 1 and "_id" in keys
                        elif isinstance(keys, list):
                            is_id_index = len(keys) == 1 and keys[0][0] == "_id"
                        
                        if is_id_index:
                            # _id indexes are automatically created by MongoDB and can't be customized
                            logger.info(f"Skipping '_id' index '{index_name}' - MongoDB creates _id indexes automatically and they can't be customized.")
                            continue
                        
                        # Check if index already exists and compare keys
                        manager = AsyncAtlasIndexManager(collection)
                        existing_index = await manager.get_index(index_name)
                        
                        if existing_index:
                            # Compare keys to see if they differ
                            tmp_keys = [(k, v) for k, v in keys.items()] if isinstance(keys, dict) else keys
                            key_doc = {k: v for k, v in tmp_keys}
                            if existing_index.get("key") != key_doc:
                                logger.warning(f"Index '{index_name}' mismatch -> drop & recreate.")
                                await manager.drop_index(index_name)
                            else:
                                logger.info(f"Regular index '{index_name}' matches; skipping.")
                                continue
                        
                        # For non-_id indexes, use background option
                        await collection.create_index(keys, name=index_name, background=True)
                        logger.info(f"Created standard index '{index_name}' on '{full_collection_name}'.")
                    except Exception as e:
                        logger.error(f"Error creating standard index '{index_name}': {e}")
    except Exception as e:
        logger.error(f"Error ensuring indexes: {e}", exc_info=True)

# --------------------------------------------------------------------------
# Core Dependencies Stubs (minimal auth for standalone)
# --------------------------------------------------------------------------
async def get_current_user(token: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """Simple auth stub - returns None (no auth required)."""
    return None

async def require_admin(current_user: Optional[Dict[str, Any]] = Depends(get_current_user)):
    """Admin check - always allows in standalone mode."""
    return current_user or {"email": "admin@standalone", "is_admin": True}

async def get_scoped_db(request: Request) -> ScopedMongoWrapper:
    """Provide scoped database wrapper."""
    if not MONGO_WRAPPER_AVAILABLE or mongo_db is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Database not available."
        )
    
    # Create scoped wrapper with experiment slug as scope
    return ScopedMongoWrapper(
        real_db=mongo_db,
        read_scopes=[SLUG],
        write_scope=SLUG
    )

# Create a minimal core_deps module
import types
core_deps = types.ModuleType("core_deps")
core_deps.get_current_user = get_current_user
core_deps.require_admin = require_admin
core_deps.get_scoped_db = get_scoped_db
sys.modules["core_deps"] = core_deps

# --------------------------------------------------------------------------
# Ray Support - REQUIRED for standalone exports (Ray is a core component)
# --------------------------------------------------------------------------
try:
    import ray
    logger.info("Ray is available.")
except ImportError:
    logger.critical("Ray is required but not available. Please install Ray: pip install ray")
    raise ImportError("Ray is required for standalone exports. Install with: pip install ray")

# Initialize Ray early at module level so type annotations can be resolved
# Ray MUST be initialized before importing experiment modules that use ray.actor.ActorHandle
RAY_CONNECTION_ADDRESS = os.getenv("RAY_CONNECTION_ADDRESS", None)
try:
    if RAY_CONNECTION_ADDRESS:
        logger.info(f"Connecting to Ray cluster at '{RAY_CONNECTION_ADDRESS}'...")
        ray.init(
            address=RAY_CONNECTION_ADDRESS,
            namespace="modular_labs",
            ignore_reinit_error=True,
            log_to_driver=False
        )
        logger.info(f"Ray connected to external cluster at '{RAY_CONNECTION_ADDRESS}'.")
    else:
        logger.info("Starting local Ray cluster at module level...")
        ray.init(
            namespace="modular_labs",
            ignore_reinit_error=True,
            log_to_driver=False,
            num_cpus=2,
            object_store_memory=2_000_000_000
        )
        logger.info("Local Ray cluster started successfully.")
except Exception as e:
    logger.error(f"CRITICAL: Failed to initialize Ray at module level: {e}", exc_info=True)
    raise RuntimeError(f"Ray initialization failed. Ray is required but could not be started: {e}") from e

# Verify Ray is initialized
if not ray.is_initialized():
    logger.critical("CRITICAL: Ray import succeeded but Ray is not initialized.")
    raise RuntimeError("Ray is required but initialization failed.")

# --------------------------------------------------------------------------
# FastAPI Application
# --------------------------------------------------------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    # Ray MUST be initialized - it was initialized at module level
    if not ray.is_initialized():
        logger.critical("CRITICAL: Ray is not initialized. Ray is required and must be initialized.")
        raise RuntimeError("Ray is required but not initialized.")
    
    app.state.ray_is_available = True
    app.state.slug_id = SLUG
    
    await connect_mongodb()
    
    # Ray is already initialized at module level, so we just create the actor
    actor_handle = None
    read_scopes = [SLUG]
    if isinstance(DB_CONFIG.get("data_scope"), list):
        read_scopes = [s if s != "self" else SLUG for s in DB_CONFIG.get("data_scope", [SLUG])]
    
    # Ray is required - create the actor
    logger.info("Ray is initialized. Creating actor...")
    try:
        actor_mod_name = f"experiments.{SLUG.replace('-', '_')}.actor"
        actor_mod = __import__(actor_mod_name, fromlist=["ExperimentActor"])
        if hasattr(actor_mod, "ExperimentActor"):
            actor_cls = getattr(actor_mod, "ExperimentActor")
            actor_name = f"{SLUG}-actor"
            
            actor_handle = actor_cls.options(
                name=actor_name,
                namespace="modular_labs",
                lifetime="detached",
                get_if_exists=True,
                max_restarts=-1
            ).remote(
                mongo_uri=MONGO_URI,
                db_name=DB_NAME,
                write_scope=SLUG,
                read_scopes=read_scopes
            )
            app.state.actor_handle = actor_handle
            logger.info(f"Ray Actor '{actor_name}' started successfully.")
        else:
            logger.error(f"CRITICAL: Experiment actor class 'ExperimentActor' not found in '{actor_mod_name}'.")
            raise RuntimeError(f"Required actor class 'ExperimentActor' not found in '{actor_mod_name}'.")
    except ModuleNotFoundError as e:
        logger.error(f"CRITICAL: Could not load actor module '{actor_mod_name}': {e}")
        raise RuntimeError(f"Required actor module not found: {e}") from e
    except Exception as e:
        logger.error(f"CRITICAL: Failed to create Ray actor: {e}", exc_info=True)
        raise RuntimeError(f"Failed to create required Ray actor: {e}") from e
    
    yield
    
    # Shutdown
    if actor_handle and app.state.ray_is_available:
        try:
            # Ray actors are detached, so we don't need to explicitly kill them
            # Ray will clean them up when the cluster shuts down
            logger.info("Ray actor will be cleaned up on cluster shutdown.")
        except Exception as e:
            logger.warning(f"Error during Ray actor cleanup: {e}")
    
    await close_mongodb()
    
    # Note: We don't shutdown Ray here because it was initialized at module level
    # Ray will be cleaned up when the process exits

app = FastAPI(
    title=f"Standalone: {SLUG}",
    version="1.0.0",
    description=f"Clean standalone FastAPI application for experiment '{SLUG}'",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan
)

# --------------------------------------------------------------------------
# Middleware: Proxy-Aware HTTPS and Experiment Scope
# --------------------------------------------------------------------------
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import ASGIApp
import re

class ProxyAwareHTTPSMiddleware(BaseHTTPMiddleware):
    """
    Proxy-aware middleware: Detects proxy headers and rewrites request.url
    to reflect the actual client scheme/host BEFORE route handlers execute.
    This ensures FastAPI's url_for() generates correct HTTPS URLs when behind proxies.
    
    Handles multiple proxy header formats:
    - X-Forwarded-Proto (Render.com, AWS ELB, etc.)
    - X-Forwarded-Ssl (older proxies)
    - Forwarded (RFC 7239)
    - X-Forwarded-Host
    """
    async def dispatch(self, request: Request, call_next: ASGIApp):
        # Store original values for debugging
        original_scheme = request.url.scheme
        original_host = request.url.hostname
        
        # Detect actual scheme from proxy headers (priority order)
        detected_scheme = original_scheme
        detected_host = original_host
        detected_port = request.url.port
        
        # Check X-Forwarded-Proto (most common - Render.com, AWS ELB)
        forwarded_proto = request.headers.get("X-Forwarded-Proto", "").lower()
        if forwarded_proto in ("https", "http"):
            detected_scheme = forwarded_proto
            logger.debug(f"Detected scheme from X-Forwarded-Proto: {detected_scheme}")
        
        # Check X-Forwarded-Ssl (older proxies)
        if request.headers.get("X-Forwarded-Ssl", "").lower() == "on":
            detected_scheme = "https"
            logger.debug(f"Detected HTTPS from X-Forwarded-Ssl header")
        
        # Check Forwarded header (RFC 7239 format: proto=https;host=example.com)
        forwarded_header = request.headers.get("Forwarded", "")
        if forwarded_header:
            # Simple parsing for proto parameter
            if "proto=https" in forwarded_header.lower():
                detected_scheme = "https"
                logger.debug(f"Detected HTTPS from Forwarded header")
            # Parse host from Forwarded header if present
            host_match = re.search(r'host=([^;,\s]+)', forwarded_header, re.IGNORECASE)
            if host_match:
                host_value = host_match.group(1).strip('"')
                if ":" in host_value:
                    detected_host, port_str = host_value.rsplit(":", 1)
                    try:
                        detected_port = int(port_str)
                    except ValueError:
                        pass
                else:
                    detected_host = host_value
        
        # Check X-Forwarded-Host
        forwarded_host = request.headers.get("X-Forwarded-Host")
        if forwarded_host:
            # X-Forwarded-Host may include port
            if ":" in forwarded_host:
                detected_host, port_str = forwarded_host.rsplit(":", 1)
                try:
                    detected_port = int(port_str)
                except ValueError:
                    pass
            else:
                detected_host = forwarded_host
            logger.debug(f"Detected host from X-Forwarded-Host: {detected_host}")
        
        # Force HTTPS in production when FORCE_HTTPS env var is set
        force_https = os.getenv("FORCE_HTTPS", "").lower() == "true"
        if force_https:
            detected_scheme = "https"
            logger.debug("Forcing HTTPS due to FORCE_HTTPS environment variable")
        
        # Store corrected values in request.state for later use
        request.state.original_scheme = original_scheme
        request.state.original_host = original_host
        request.state.detected_scheme = detected_scheme
        request.state.detected_host = detected_host
        request.state.detected_port = detected_port
        
        # Rewrite request.scope to reflect actual client scheme/host
        # This ensures request.url and url_for() use correct values
        if detected_scheme != original_scheme or detected_host != original_host:
            # Modify the ASGI scope to change URL components
            request.scope["scheme"] = detected_scheme
            # Update server tuple (host, port)
            if detected_port:
                request.scope["server"] = (detected_host, detected_port)
            else:
                # Use default port based on scheme
                default_port = 443 if detected_scheme == "https" else 80
                request.scope["server"] = (detected_host, default_port)
            
            # Reconstruct the URL object
            # Note: Starlette's Request.url is cached, so we need to clear it
            if hasattr(request, "_url"):
                delattr(request, "_url")
            
            logger.info(
                f"Proxy-aware HTTPS: Rewrote request URL "
                f"{original_scheme}://{original_host} -> {detected_scheme}://{detected_host}"
            )
        
        response = await call_next(request)
        return response

class HTTPSEnforcementMiddleware(BaseHTTPMiddleware):
    """
    Proxy-aware security middleware: Only enforces HTTPS when the request actually
    came via HTTPS (detected through proxy headers or direct connection).
    
    This is intelligent for deployments behind proxies like Render.com:
    - If the proxy indicates HTTPS (X-Forwarded-Proto: https), enforces HTTPS
    - If the proxy indicates HTTP, does NOT enforce HTTPS (allows proxy to handle it)
    - Prevents HTTP downgrade attacks and mixed content issues only when HTTPS is active.
    """
    async def dispatch(self, request: Request, call_next: ASGIApp):
        response = await call_next(request)
        
        # Check if this request is actually using HTTPS
        # Use the detected scheme from ProxyAwareHTTPSMiddleware if available
        # Otherwise check the request URL scheme directly
        detected_scheme = getattr(request.state, "detected_scheme", None)
        if detected_scheme is None:
            # Fallback: check proxy headers directly
            forwarded_proto = request.headers.get("X-Forwarded-Proto", "").lower()
            if forwarded_proto in ("https", "http"):
                detected_scheme = forwarded_proto
            elif request.headers.get("X-Forwarded-Ssl", "").lower() == "on":
                detected_scheme = "https"
            elif "proto=https" in request.headers.get("Forwarded", "").lower():
                detected_scheme = "https"
            else:
                detected_scheme = request.url.scheme
        
        # Only enforce HTTPS if the request actually came via HTTPS
        # This allows the proxy (like Render.com) to handle HTTPS termination
        # without our app trying to force it when it shouldn't
        is_https = detected_scheme == "https"
        
        if not is_https:
            # Request came via HTTP - don't enforce HTTPS
            # This allows proper operation behind proxies that handle HTTPS at the edge
            logger.debug(f"Skipping HTTPS enforcement - request came via {detected_scheme}")
            return response
        
        # Request came via HTTPS - enforce it
        logger.debug("Enforcing HTTPS - request came via HTTPS")
        
        # Add HSTS header - tells browsers to always use HTTPS for this domain
        # Only add this when we're actually serving HTTPS
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains; preload"
        
        # Force HTTPS on any Location redirect headers - catch ANY http:// redirect
        if "Location" in response.headers:
            location = response.headers["Location"]
            # Replace http:// with https:// directly (simple string replacement)
            if location.startswith("http://"):
                https_location = location.replace("http://", "https://", 1)
                response.headers["Location"] = https_location
                logger.debug(f"Enforced HTTPS redirect: {location} -> {https_location}")
        
        # Sanitize mixed content in response bodies
        content_type = response.headers.get("content-type", "").lower()
        
        # Only process text-based content types
        text_content_types = [
            "application/json",
            "text/html",
            "text/css",
            "text/javascript",
            "application/javascript",
            "text/plain",
            "text/xml",
            "application/xml",
        ]
        
        if any(ct in content_type for ct in text_content_types):
            # Check if response has a body
            if hasattr(response, "body") and response.body:
                try:
                    # Decode response body
                    if isinstance(response.body, bytes):
                        body_text = response.body.decode("utf-8")
                    else:
                        body_text = str(response.body)
                    
                    original_body = body_text
                    
                    # Replace http:// URLs with https://
                    # Pattern 1: Quoted URLs in JSON/HTML attributes: "http://...
                    body_text = re.sub(r'"http://', '"https://', body_text)
                    
                    # Pattern 2: Unquoted URLs: http://...
                    body_text = re.sub(r'\bhttp://', 'https://', body_text)
                    
                    # Pattern 3: HTML/CSS attributes: src="http://, href="http://, url(http://
                    body_text = re.sub(r'(src|href)=["\']http://', r'\1="https://', body_text)
                    body_text = re.sub(r'url\(http://', 'url(https://', body_text)
                    
                    # Pattern 4: JSON string values (more specific)
                    body_text = re.sub(r'":\s*"http://', '": "https://', body_text)
                    
                    # Only update if changes were made
                    if body_text != original_body:
                        # Re-encode as bytes
                        response.body = body_text.encode("utf-8")
                        # Update content length if present
                        if "content-length" in response.headers:
                            response.headers["content-length"] = str(len(response.body))
                        
                        logger.debug("Sanitized mixed content in response body (HTTP -> HTTPS)")
                
                except (UnicodeDecodeError, AttributeError) as e:
                    # Skip if body isn't text or can't be decoded
                    logger.debug(f"Skipping mixed content sanitization: {e}")
        
        return response

class ExperimentScopeMiddleware(BaseHTTPMiddleware):
    """Middleware to set slug_id for experiment routes."""
    async def dispatch(self, request: Request, call_next: ASGIApp):
        path = request.url.path
        if path.startswith(f"/experiments/{SLUG}"):
            request.state.slug_id = SLUG
        else:
            request.state.slug_id = None
        response = await call_next(request)
        return response

# Middleware order matters: Proxy-aware middleware must run FIRST
# so that request.url is corrected before route handlers execute
app.add_middleware(ProxyAwareHTTPSMiddleware)
app.add_middleware(ExperimentScopeMiddleware)

# Add proxy-aware HTTPS enforcement in production
# The middleware will only enforce HTTPS when requests actually come via HTTPS
# (detected from proxy headers like X-Forwarded-Proto)
G_NOME_ENV = os.getenv("G_NOME_ENV", "production").lower()
if G_NOME_ENV == "production":
    logger.info("Production environment detected. Enabling proxy-aware HTTPSEnforcementMiddleware.")
    logger.info("HTTPS will only be enforced when requests actually come via HTTPS (detected from proxy headers).")
    app.add_middleware(HTTPSEnforcementMiddleware)
else:
    logger.warning(f"Non-production environment ('{G_NOME_ENV}') detected. Skipping HTTPS enforcement.")

# Mount static files
static_dir = EXPERIMENT_DIR / "static"
if static_dir.is_dir():
    app.mount(
        f"/experiments/{SLUG}/static",
        StaticFiles(directory=str(static_dir)),
        name=f"{SLUG}_static"
    )
    logger.info(f"Mounted static files from: {static_dir}")

# Root route - serve experiment index.html
@app.get("/", response_class=HTMLResponse)
async def standalone_index(request: Request):
    """Serve index.html from the experiment templates."""
    local_index = EXPERIMENT_DIR / "templates" / "index.html"
    if local_index.is_file():
        templates = Jinja2Templates(directory=str(EXPERIMENT_DIR / "templates"))
        return templates.TemplateResponse("index.html", {"request": request})
    
    # Fallback
    return HTMLResponse(f"""
    <html>
        <head><title>Standalone: {SLUG}</title></head>
        <body>
            <h2>Standalone experiment: '{SLUG}'</h2>
            <p>No local index.html found. Check templates directory.</p>
            <p><a href="/docs">API Documentation</a></p>
        </body>
    </html>
    """, status_code=200)

# Debug endpoint
@app.get("/_debug/db", response_class=JSONResponse)
async def debug_db(request: Request):
    """Debug endpoint to check database status."""
    if mongo_db is None:
        return {"error": "Database not connected"}
    
    collections_info = {}
    for collection_name in await mongo_db.list_collection_names():
        collection = mongo_db[collection_name]
        count = await collection.count_documents({})
        collections_info[collection_name] = {"count": count}
    
    return {
        "mongo_uri": MONGO_URI,
        "db_name": DB_NAME,
        "collections": collections_info,
        "ray_initialized": ray.is_initialized() and getattr(request.app.state, "ray_is_available", False),
        "ray_required": True
    }

# --------------------------------------------------------------------------
# Import and Mount Experiment Router
# --------------------------------------------------------------------------
sys.path.insert(0, str(BASE_DIR))
try:
    mod_name = f"experiments.{SLUG.replace('-', '_')}"
    exp_mod = __import__(mod_name, fromlist=["bp"])
    if not hasattr(exp_mod, "bp"):
        raise RuntimeError(f"Module '{mod_name}' has no 'bp' APIRouter.")
    exp_router = getattr(exp_mod, "bp")
    app.include_router(exp_router, prefix=f"/experiments/{SLUG}", tags=[f"Experiment: {SLUG}"])
    logger.info(f"Included APIRouter for experiment '{SLUG}'.")
except ModuleNotFoundError as e:
    logger.error(f"No local experiment module found for '{SLUG}': {e}")
except Exception as e:
    logger.error(f"Failed to include experiment router: {e}", exc_info=True)

# --------------------------------------------------------------------------
# Main Entry Point
# --------------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    logger.info(f"Starting standalone server for experiment '{SLUG}'...")
    logger.info(f"MongoDB URI: {MONGO_URI}")
    logger.info(f"Database: {DB_NAME}")
    logger.info(f"Port: {PORT}")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=PORT,
        log_level="info",
        reload=False,
        proxy_headers=True,
        forwarded_allow_ips="*"
    )
