#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Intelligent Standalone FastAPI Application for experiment '{{ slug_id }}'.

This is a clean, production-ready FastAPI application that:
- Uses real MongoDB (Motor) for persistent storage
- REQUIRES Ray - Ray is a core component and must be available
- Includes proper database initialization and index management
- Supports experiment-specific routes and static files

Environment Variables:
- MONGO_URI: MongoDB connection string (default: mongodb://mongo:27017/)
- DB_NAME: Database name (default: labs_db)
- PORT: Server port (default: 8000)
- LOG_LEVEL: Logging level (default: INFO)
- RAY_CONNECTION_ADDRESS: (Optional) Ray cluster address. If not set, starts local Ray.
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request, HTTPException, status, Depends
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

# MongoDB imports
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase

# Import experiment-specific dependencies
try:
    from async_mongo_wrapper import ScopedMongoWrapper, AsyncAtlasIndexManager
    MONGO_WRAPPER_AVAILABLE = True
except ImportError:
    MONGO_WRAPPER_AVAILABLE = False
    logging.warning("async_mongo_wrapper not available. Using direct MongoDB access.")

# --------------------------------------------------------------------------
# Logging Configuration
# --------------------------------------------------------------------------
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO").upper(),
    format="%(asctime)s | standalone | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("standalone")

# --------------------------------------------------------------------------
# Constants and Paths
# --------------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent
SLUG = "{{ slug_id }}"
EXPERIMENT_DIR = BASE_DIR / "experiments" / SLUG

# --------------------------------------------------------------------------
# Environment Configuration
# --------------------------------------------------------------------------
MONGO_URI = os.getenv("MONGO_URI", "mongodb://mongo:27017/")
DB_NAME = os.getenv("DB_NAME", "labs_db")
PORT = int(os.getenv("PORT", "8000"))

# --------------------------------------------------------------------------
# Load Configuration from JSON files
# --------------------------------------------------------------------------
def _load_json(path: Path, default: Any) -> Any:
    try:
        with path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        logger.warning(f"JSON file not found: {path.name}. Using default.")
    except Exception as e:
        logger.error(f"Failed to read '{path.name}': {e}")
    return default

DB_CONFIG: Dict[str, Any] = _load_json(BASE_DIR / "db_config.json", {})
DB_COLLECTIONS: Dict[str, List[Dict[str, Any]]] = _load_json(BASE_DIR / "db_collections.json", {})

# --------------------------------------------------------------------------
# MongoDB Connection and Database Setup
# --------------------------------------------------------------------------
mongo_client: Optional[AsyncIOMotorClient] = None
mongo_db: Optional[AsyncIOMotorDatabase] = None

async def connect_mongodb():
    """Initialize MongoDB connection."""
    global mongo_client, mongo_db
    try:
        logger.info(f"Connecting to MongoDB at '{MONGO_URI}'...")
        mongo_client = AsyncIOMotorClient(
            MONGO_URI,
            serverSelectionTimeoutMS=5000,
            appname=f"Standalone-{SLUG}"
        )
        await mongo_client.admin.command("ping")
        mongo_db = mongo_client[DB_NAME]
        logger.info(f"MongoDB connection successful (Database: '{DB_NAME}').")
        
        # Seed database with exported data if collections are empty
        await seed_database()
        
        # Ensure indexes from manifest
        await ensure_indexes()
        
    except Exception as e:
        logger.critical(f"CRITICAL ERROR: Failed to connect to MongoDB: {e}", exc_info=True)
        raise RuntimeError(f"MongoDB connection failed: {e}") from e

async def close_mongodb():
    """Close MongoDB connection."""
    global mongo_client
    if mongo_client is not None:
        mongo_client.close()
        logger.info("MongoDB connection closed.")

async def seed_database():
    """Seed database collections with exported data."""
    if mongo_db is None:
        return
    
    for collection_name, docs in DB_COLLECTIONS.items():
        try:
            collection = mongo_db[collection_name]
            existing_count = await collection.count_documents({})
            if existing_count == 0 and docs:
                logger.info(f"Seeding collection '{collection_name}' with {len(docs)} documents...")
                # Convert string _id back to ObjectId if needed
                for doc in docs:
                    if "_id" in doc and isinstance(doc["_id"], str):
                        try:
                            from bson import ObjectId
                            doc["_id"] = ObjectId(doc["_id"])
                        except Exception:
                            pass  # Keep as string if ObjectId conversion fails
                await collection.insert_many(docs)
                logger.info(f"Successfully seeded '{collection_name}'.")
        except Exception as e:
            logger.error(f"Error seeding collection '{collection_name}': {e}", exc_info=True)

async def ensure_indexes():
    """Ensure MongoDB indexes from experiment manifest."""
    if mongo_db is None or not MONGO_WRAPPER_AVAILABLE:
        return
    
    if "managed_indexes" not in DB_CONFIG:
        return
    
    try:
        managed_indexes = DB_CONFIG.get("managed_indexes", {})
        for collection_name, index_defs in managed_indexes.items():
            # Construct the full collection name (with experiment prefix)
            full_collection_name = f"{SLUG}_{collection_name}"
            collection = mongo_db[full_collection_name]
            
            for index_def in index_defs:
                base_index_name = index_def.get("name", "unnamed_index")
                # Prefix index name with experiment slug to match actor's expectation
                index_name = f"{SLUG}_{base_index_name}"
                index_type = index_def.get("type", "search")
                definition = index_def.get("definition", {})
                
                if index_type == "vectorSearch":
                    # Use AsyncAtlasIndexManager for vector search indexes
                    manager = AsyncAtlasIndexManager(collection)
                    
                    # Check if index already exists
                    existing_index = await manager.get_search_index(index_name)
                    if existing_index:
                        current_def = existing_index.get("latestDefinition", existing_index.get("definition"))
                        
                        # Normalize both definitions for order-insensitive comparison
                        def _normalize_json_def(obj):
                            try:
                                return json.loads(json.dumps(obj, sort_keys=True))
                            except (TypeError, ValueError):
                                return obj
                        
                        normalized_current = _normalize_json_def(current_def)
                        normalized_expected = _normalize_json_def(definition)
                        
                        if normalized_current == normalized_expected:
                            logger.info(f"Vector search index '{index_name}' definition matches. Already exists.")
                            if not existing_index.get("queryable") and existing_index.get("status") != "FAILED":
                                logger.info(f"Index '{index_name}' not queryable yet; waiting...")
                                await manager._wait_for_search_index_ready(index_name, 30)
                        else:
                            # Index definition changed - update it
                            current_paths = [f.get('path', '?') for f in (normalized_current.get('fields', []) if isinstance(normalized_current, dict) else []) if isinstance(f, dict)]
                            expected_paths = [f.get('path', '?') for f in (normalized_expected.get('fields', []) if isinstance(normalized_expected, dict) else []) if isinstance(f, dict)]
                            logger.warning(f"Vector search index '{index_name}' definition changed. Updating...")
                            logger.info(f"Current filter fields: {current_paths}")
                            logger.info(f"Expected filter fields: {expected_paths}")
                            await manager.update_search_index(name=index_name, definition=definition, wait_for_ready=True)
                            logger.info(f"✅ Updated vector search index '{index_name}'.")
                    else:
                        # Index doesn't exist - create it
                        await manager.create_search_index(
                            name=index_name,
                            definition=definition,
                            index_type="vectorSearch",
                            wait_for_ready=True
                        )
                        logger.info(f"Created vector search index '{index_name}' on '{full_collection_name}'.")
                elif index_type == "search":
                    # Lucene search index
                    manager = AsyncAtlasIndexManager(collection)
                    
                    # Check if index already exists
                    existing_index = await manager.get_search_index(index_name)
                    if existing_index:
                        current_def = existing_index.get("latestDefinition", existing_index.get("definition"))
                        
                        # Normalize both definitions for order-insensitive comparison
                        def _normalize_json_def(obj):
                            try:
                                return json.loads(json.dumps(obj, sort_keys=True))
                            except (TypeError, ValueError):
                                return obj
                        
                        normalized_current = _normalize_json_def(current_def)
                        normalized_expected = _normalize_json_def(definition)
                        
                        if normalized_current == normalized_expected:
                            logger.info(f"Search index '{index_name}' definition matches. Already exists.")
                            if not existing_index.get("queryable") and existing_index.get("status") != "FAILED":
                                logger.info(f"Index '{index_name}' not queryable yet; waiting...")
                                await manager._wait_for_search_index_ready(index_name, 30)
                        else:
                            # Index definition changed - update it
                            logger.warning(f"Search index '{index_name}' definition changed. Updating...")
                            await manager.update_search_index(name=index_name, definition=definition, wait_for_ready=True)
                            logger.info(f"✅ Updated search index '{index_name}'.")
                    else:
                        # Index doesn't exist - create it
                        await manager.create_search_index(
                            name=index_name,
                            definition=definition,
                            index_type="search",
                            wait_for_ready=True
                        )
                        logger.info(f"Created search index '{index_name}' on '{full_collection_name}'.")
                else:
                    # Standard MongoDB index - use prefixed index name
                    try:
                        keys = index_def.get("keys", {})
                        
                        # Check if this is an _id index (MongoDB creates these automatically)
                        is_id_index = False
                        if isinstance(keys, dict):
                            is_id_index = len(keys) == 1 and "_id" in keys
                        elif isinstance(keys, list):
                            is_id_index = len(keys) == 1 and keys[0][0] == "_id"
                        
                        if is_id_index:
                            # _id indexes are automatically created by MongoDB and can't be customized
                            logger.info(f"Skipping '_id' index '{index_name}' - MongoDB creates _id indexes automatically and they can't be customized.")
                            continue
                        
                        # Check if index already exists and compare keys
                        manager = AsyncAtlasIndexManager(collection)
                        existing_index = await manager.get_index(index_name)
                        
                        if existing_index:
                            # Compare keys to see if they differ
                            tmp_keys = [(k, v) for k, v in keys.items()] if isinstance(keys, dict) else keys
                            key_doc = {k: v for k, v in tmp_keys}
                            if existing_index.get("key") != key_doc:
                                logger.warning(f"Index '{index_name}' mismatch -> drop & recreate.")
                                await manager.drop_index(index_name)
                            else:
                                logger.info(f"Regular index '{index_name}' matches; skipping.")
                                continue
                        
                        # For non-_id indexes, use background option
                        await collection.create_index(keys, name=index_name, background=True)
                        logger.info(f"Created standard index '{index_name}' on '{full_collection_name}'.")
                    except Exception as e:
                        logger.error(f"Error creating standard index '{index_name}': {e}")
    except Exception as e:
        logger.error(f"Error ensuring indexes: {e}", exc_info=True)

# --------------------------------------------------------------------------
# Core Dependencies Stubs (minimal auth for standalone)
# --------------------------------------------------------------------------
async def get_current_user(token: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """Simple auth stub - returns None (no auth required)."""
    return None

async def require_admin(current_user: Optional[Dict[str, Any]] = Depends(get_current_user)):
    """Admin check - always allows in standalone mode."""
    return current_user or {"email": "admin@standalone", "is_admin": True}

async def get_scoped_db(request: Request) -> ScopedMongoWrapper:
    """Provide scoped database wrapper."""
    if not MONGO_WRAPPER_AVAILABLE or mongo_db is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Database not available."
        )
    
    # Create scoped wrapper with experiment slug as scope
    return ScopedMongoWrapper(
        real_db=mongo_db,
        read_scopes=[SLUG],
        write_scope=SLUG
    )

# Create a minimal core_deps module
import types
core_deps = types.ModuleType("core_deps")
core_deps.get_current_user = get_current_user
core_deps.require_admin = require_admin
core_deps.get_scoped_db = get_scoped_db
sys.modules["core_deps"] = core_deps

# --------------------------------------------------------------------------
# Ray Support - REQUIRED for standalone exports (Ray is a core component)
# --------------------------------------------------------------------------
try:
    import ray
    logger.info("Ray is available.")
except ImportError:
    logger.critical("Ray is required but not available. Please install Ray: pip install ray")
    raise ImportError("Ray is required for standalone exports. Install with: pip install ray")

# Initialize Ray early at module level so type annotations can be resolved
# Ray MUST be initialized before importing experiment modules that use ray.actor.ActorHandle
RAY_CONNECTION_ADDRESS = os.getenv("RAY_CONNECTION_ADDRESS", None)
try:
    if RAY_CONNECTION_ADDRESS:
        logger.info(f"Connecting to Ray cluster at '{RAY_CONNECTION_ADDRESS}'...")
        ray.init(
            address=RAY_CONNECTION_ADDRESS,
            namespace="modular_labs",
            ignore_reinit_error=True,
            log_to_driver=False
        )
        logger.info(f"Ray connected to external cluster at '{RAY_CONNECTION_ADDRESS}'.")
    else:
        logger.info("Starting local Ray cluster at module level...")
        ray.init(
            namespace="modular_labs",
            ignore_reinit_error=True,
            log_to_driver=False,
            num_cpus=2,
            object_store_memory=2_000_000_000
        )
        logger.info("Local Ray cluster started successfully.")
except Exception as e:
    logger.error(f"CRITICAL: Failed to initialize Ray at module level: {e}", exc_info=True)
    raise RuntimeError(f"Ray initialization failed. Ray is required but could not be started: {e}") from e

# Verify Ray is initialized
if not ray.is_initialized():
    logger.critical("CRITICAL: Ray import succeeded but Ray is not initialized.")
    raise RuntimeError("Ray is required but initialization failed.")

# --------------------------------------------------------------------------
# FastAPI Application
# --------------------------------------------------------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    # Ray MUST be initialized - it was initialized at module level
    if not ray.is_initialized():
        logger.critical("CRITICAL: Ray is not initialized. Ray is required and must be initialized.")
        raise RuntimeError("Ray is required but not initialized.")
    
    app.state.ray_is_available = True
    app.state.slug_id = SLUG
    
    await connect_mongodb()
    
    # Ray is already initialized at module level, so we just create the actor
    actor_handle = None
    read_scopes = [SLUG]
    if isinstance(DB_CONFIG.get("data_scope"), list):
        read_scopes = [s if s != "self" else SLUG for s in DB_CONFIG.get("data_scope", [SLUG])]
    
    # Ray is required - create the actor
    logger.info("Ray is initialized. Creating actor...")
    try:
        actor_mod_name = f"experiments.{SLUG.replace('-', '_')}.actor"
        actor_mod = __import__(actor_mod_name, fromlist=["ExperimentActor"])
        if hasattr(actor_mod, "ExperimentActor"):
            actor_cls = getattr(actor_mod, "ExperimentActor")
            actor_name = f"{SLUG}-actor"
            
            actor_handle = actor_cls.options(
                name=actor_name,
                namespace="modular_labs",
                lifetime="detached",
                get_if_exists=True,
                max_restarts=-1
            ).remote(
                mongo_uri=MONGO_URI,
                db_name=DB_NAME,
                write_scope=SLUG,
                read_scopes=read_scopes
            )
            app.state.actor_handle = actor_handle
            logger.info(f"Ray Actor '{actor_name}' started successfully.")
        else:
            logger.error(f"CRITICAL: Experiment actor class 'ExperimentActor' not found in '{actor_mod_name}'.")
            raise RuntimeError(f"Required actor class 'ExperimentActor' not found in '{actor_mod_name}'.")
    except ModuleNotFoundError as e:
        logger.error(f"CRITICAL: Could not load actor module '{actor_mod_name}': {e}")
        raise RuntimeError(f"Required actor module not found: {e}") from e
    except Exception as e:
        logger.error(f"CRITICAL: Failed to create Ray actor: {e}", exc_info=True)
        raise RuntimeError(f"Failed to create required Ray actor: {e}") from e
    
    yield
    
    # Shutdown
    if actor_handle and app.state.ray_is_available:
        try:
            # Ray actors are detached, so we don't need to explicitly kill them
            # Ray will clean them up when the cluster shuts down
            logger.info("Ray actor will be cleaned up on cluster shutdown.")
        except Exception as e:
            logger.warning(f"Error during Ray actor cleanup: {e}")
    
    await close_mongodb()
    
    # Note: We don't shutdown Ray here because it was initialized at module level
    # Ray will be cleaned up when the process exits

app = FastAPI(
    title=f"Standalone: {SLUG}",
    version="1.0.0",
    description=f"Clean standalone FastAPI application for experiment '{SLUG}'",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan
)

# --------------------------------------------------------------------------
# Middleware to set slug_id for experiment routes
# --------------------------------------------------------------------------
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import ASGIApp

class ExperimentScopeMiddleware(BaseHTTPMiddleware):
    """Middleware to set slug_id for experiment routes."""
    async def dispatch(self, request: Request, call_next: ASGIApp):
        path = request.url.path
        if path.startswith(f"/experiments/{SLUG}"):
            request.state.slug_id = SLUG
        else:
            request.state.slug_id = None
        response = await call_next(request)
        return response

app.add_middleware(ExperimentScopeMiddleware)

# Mount static files
static_dir = EXPERIMENT_DIR / "static"
if static_dir.is_dir():
    app.mount(
        f"/experiments/{SLUG}/static",
        StaticFiles(directory=str(static_dir)),
        name=f"{SLUG}_static"
    )
    logger.info(f"Mounted static files from: {static_dir}")

# Root route - serve experiment index.html
@app.get("/", response_class=HTMLResponse)
async def standalone_index(request: Request):
    """Serve index.html from the experiment templates."""
    local_index = EXPERIMENT_DIR / "templates" / "index.html"
    if local_index.is_file():
        templates = Jinja2Templates(directory=str(EXPERIMENT_DIR / "templates"))
        return templates.TemplateResponse("index.html", {"request": request})
    
    # Fallback
    return HTMLResponse(f"""
    <html>
        <head><title>Standalone: {SLUG}</title></head>
        <body>
            <h2>Standalone experiment: '{SLUG}'</h2>
            <p>No local index.html found. Check templates directory.</p>
            <p><a href="/docs">API Documentation</a></p>
        </body>
    </html>
    """, status_code=200)

# Debug endpoint
@app.get("/_debug/db", response_class=JSONResponse)
async def debug_db(request: Request):
    """Debug endpoint to check database status."""
    if mongo_db is None:
        return {"error": "Database not connected"}
    
    collections_info = {}
    for collection_name in await mongo_db.list_collection_names():
        collection = mongo_db[collection_name]
        count = await collection.count_documents({})
        collections_info[collection_name] = {"count": count}
    
    return {
        "mongo_uri": MONGO_URI,
        "db_name": DB_NAME,
        "collections": collections_info,
        "ray_initialized": ray.is_initialized() and getattr(request.app.state, "ray_is_available", False),
        "ray_required": True
    }

# --------------------------------------------------------------------------
# Import and Mount Experiment Router
# --------------------------------------------------------------------------
sys.path.insert(0, str(BASE_DIR))
try:
    mod_name = f"experiments.{SLUG.replace('-', '_')}"
    exp_mod = __import__(mod_name, fromlist=["bp"])
    if not hasattr(exp_mod, "bp"):
        raise RuntimeError(f"Module '{mod_name}' has no 'bp' APIRouter.")
    exp_router = getattr(exp_mod, "bp")
    app.include_router(exp_router, prefix=f"/experiments/{SLUG}", tags=[f"Experiment: {SLUG}"])
    logger.info(f"Included APIRouter for experiment '{SLUG}'.")
except ModuleNotFoundError as e:
    logger.error(f"No local experiment module found for '{SLUG}': {e}")
except Exception as e:
    logger.error(f"Failed to include experiment router: {e}", exc_info=True)

# --------------------------------------------------------------------------
# Main Entry Point
# --------------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    logger.info(f"Starting standalone server for experiment '{SLUG}'...")
    logger.info(f"MongoDB URI: {MONGO_URI}")
    logger.info(f"Database: {DB_NAME}")
    logger.info(f"Port: {PORT}")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=PORT,
        log_level="info",
        reload=False
    )
